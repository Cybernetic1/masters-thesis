\begin{preface}

Our main contribution is the re-formulation of old-fashioned logic-based AI based on the new technique of deep learning.

Deep learning is truly revolutionary for a single reason:  It learns much faster.  This allows to handle huge datasets which were perviously out of consideration, and crucially, common-sense reasoning by its nature has to be training on massive datasets.  The reason why we're now seeing machines with human-comparable intelligence owes to this feat.  

Why is deep learning so highly efficient?  It seems to achieve this by operating in a ``continuous'' domain (by gradient descent) in very high-dimensional spaces, such that the problem of \textbf{local minima} seems to be miraculously avoided.  To accurately explain this phenomenon may involve the mysteries of the P $\stackrel{?}{=}$ NP problem and should not be expected any time soon.

Deep learning also has the characteristic of hierarchically many layers of parameters, but this may not be a necessary requirement, as recent thinking on ``\textbf{shallow learning}'' suggests.  Our algorithm also challenges this view.

Our key contribution is to make logic rules \textbf{differentiable}, so that gradient descent can be applied to find an optimal set of logic rules optimizing some objective function.  This solves the age-old problem of logic-based AI's lack of an efficient learning algorithm, that is responsible for the so-called ``\textbf{AI Winter}'' around the 1970-80s.

The Transformer, based on Self-Attention, is the current state-of-the-art module for building LLMs (Large Language Models).  It is based on a \textbf{differentiable memory-retrieval mechanism} that originated in \textbf{Neural Turing Machines}, and later shown to be equivalent to \textbf{Hopfield Networks}, also a kind of associative memory.  It suffers from the drawback that it is not easy to interpret what the ``tokens'' stand for, that hinders further attempts to improve it.  The logic-based approach offers a much better understanding of the internal workings of AGI.

Moreover, our model, which may be called ``Logic Transformer,'' is significantly more structured than the traditional Transformer.  According to the \textbf{No Free Lunch theorem}, machines with more restrictive structures have more inductive bias and learn faster.

The most important theoretical background that inspired this research is Paul Halmos' \textbf{algebraic logic}.  In retrospect, I just needed to make traditional logic algorithms differentiable.  The solution I found is very straight-forward, or may even seem ``high-school-ish.''  But with a great mathematician as Halmos standing behind my back and pointing to what needs to be done, I felt much more confident to work through some tedious details.  One cannot exactly pinpoint what is the importance of ``beautiful'' mathematics, but it really made a big difference to me.

In 2019 Richard Sutton wrote:
\begin{quotation}
	\textit{... the bitter lesson that building in \textbf{how we think we think} does not work in the long run... 70 years of AI research [had shown] that general methods that leverage computation are ultimately the most effective, and by a large margin.}
\end{quotation}
My research direction of applying \textbf{logic structure} as inductive bias to speed up learning, seems to go directly against his advice.  I came from a logic-based AI background, so it was very natural for me to try to ``graft'' those ideas onto the new paradigm of deep learning.  As I worked on this direction, I discovered that logic seems to have quite strong structural biases, which is a good thing for accelerating learning.  Last month, I made use of the \textbf{dihedral symmetry} of the TicTacToe board and succeeded to write a reinforcement learner that learns a near-optimal score in just 40 seconds (other methods without exploiting symmetry usually take hours).  Such is the power of \textbf{symmetries} to slash the search space.  But I cannot find a rigorous argument to say that this is indeed the better approach, mainly because we know so little about the search space of AGIs.  Crucially, I cannot say for sure that the big family of symbolic logics discovered by us humans are the most effective way to describe this world.  We know that certain mathematical structures are highly efficient, such as linear algebra, or Fourier methods with their ``spectral efficiency,'' and they may offer representations more efficient than our symbolic logics.  The reader is encouraged to explore those directions.

Good luck.

\begin{flushright}
YKY\\
2024, Hong Kong
\end{flushright}

\end{preface}

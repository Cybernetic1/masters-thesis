\chapter{Design of algorithm}\label{chap:algorithm}

The following table depicts the main correspondences relevant to our research:
\begin{equation}
\begin{tabular}{|c|c|c|}
\hline
\textbf{LOGIC} & \textbf{facts} & \textbf{rules} \\
& \mbox{human(socrates)} & $\forall x. \mbox{human}(x) \rightarrow \mbox{mortal}(x)$ \\
\hline
\textbf{ALGEBRA} & \textbf{element} & \textbf{element} \\
& $p \in \mathbb{A} $ & $(p \rightarrow q) \in \mathbb{A} $ \\
\hline
\textbf{WORLD} & \textbf{states} & \textbf{state transitions} \\
& $x_t$ & $x_t \stackrel{\squareF}{\mapsto} x_{t+1}$ \\
\hline
\end{tabular}
\end{equation}
The relation between LOGIC and WORLD has been elucidated quite thoroughly in the AI literature.  Note that the state $x_t$ is made up of a set of facts (logic propositions).  A single step of logic inference results in a new conclusion $\delta x$ which is \textit{added} (as a set element) to the current state $x_t$ to form a new state $x_{t+1}$.  Here $t$ refers to ``mental time'' which does not necessarily coincide with real time.

\section{From abstract algebraic logic to concrete computations}

There are two main routes to make abstract algebraic logic concrete:
\begin{itemize}
	\item Find \textbf{matrix representations} of the logical algebra
	\item Implement the logical algebra as the commutative algebra of (classical) \textbf{polynomials}
\end{itemize}

\section{What does it mean to train the AI?}

From the previous section,
\begin{equation}
F(x) = 0 \quad \mbox{is the solution to} \quad \dot{x} = f(x)
\end{equation}
and the two descriptions (by $F$ or by $f$) are equivalent.

The discrete version of $f$ is $\squaref$:
\begin{equation}
	\squaref(x_t) = x_{t+1} - x_t = \delta x
\end{equation}

The sensory data from the AI are a set of ``world'' points $\{ x_i \}$  and we require either:
\begin{equation}
F(x_t) = 0 \quad \mbox{or} \quad \squaref(x_t) = \delta x = x_{t+1} - x_t
\end{equation}
and $F$ or $f$ can be trained by gradient descent to eliminate errors in the above conditions (equations).

\begin{itemize}
	\item the $x_t$'s are represented as \textbf{logic facts}
	\item $F$ or $f$ is represented as \textbf{logic rules}
\end{itemize}
and we need to \textbf{evaluate} $F(x_t)$ or $f(x_t)$.

Let's do some examples:
% \usetagform{hidden}
\begin{equation}
\begin{tabular}{|c|c|}
\hline
\textbf{Logic formula} & \textbf{Algebraic form} \\
\hline
human(socrates) & h(s) = 1 \\
\hline
human(socrates) $\wedge$ human(plato) & h(s) $\cdot$ h(p) = 1 \\
\hline
human(socrates) $\rightarrow$ mortal(socrates) &  1 + h(s) + h(s) $\cdot$ m(s) = 1 \\
\hline
$\forall x.$ human($x$) & $h(x)$ is a propositional function \\
& $\forall_x h(x)$ is a constant function mapping to 1 or 0 \\
\hline
$\forall x.$ human($x$) $\rightarrow$ mortal($x$) & $\forall_x \bigl( 1 + h(x) + h(x) \cdot m(x) \bigr) $ \\
& is a constant function mapping to 1 or 0 \\
\hline
$\forall x,y,z.$ father($x,y$) $\wedge$ father($y,z$) $\rightarrow$ & $\forall_x \forall_y \forall_z \bigl( 1 + f(x,y) \cdot f(y,z) + f(x,y) \cdot f(y,z) \cdot g(x,z) \bigr) $ \\
grandfather($x,z$) & $\mapsto$ 0 or 1 \\
\hline
\textbf{general Horn formula}: & $\forall_{x...} \big( 1 + P \cdot Q \cdot R .... + P \cdot Q \cdot R ... \cdot Z \big) $ \\
$\forall_{x...} P \wedge Q \wedge R ... \rightarrow Z$ & $\mapsto$ 0 or 1 \\
\hline
\end{tabular}
\label{table:formulas}
\end{equation}

Now imagine there are millions of such rules.  Number of predicates obviously increases.

Does each equation require new variables, or can variables be re-used? Seems yes, can be re-used.

The \textbf{loss function} would be the sum of squared errors over all equations:
\begin{equation}
\mathcal{L} = \sum_{\mathrm{eqns}} \epsilon^2 = \sum_i \big( \phi_i (x...) - 1 \big)^2 .
\label{loss-function}
\end{equation}
Learning means to perform the \textbf{gradient descent} via $ \nabla_\Phi \mathcal{L} = \frac{\partial \mathcal{L}}{\partial \Phi} $ where $\Phi$ is the set of parameters for the equations.

Potential problems:
\begin{itemize}
	\item How to represent the set of equations efficiently?  Matrix of coefficients seems wasteful.
	\item We have lost the ``\textbf{deepness}'' of deep learning, but there is recent research showing that \textbf{shallow learning} may work well too.
	\item Need to iterate logical inference multiple times using the same set of equations.
\end{itemize}

How are new conclusions added to the state?  What is the state?  State = set of facts = set of \textbf{grounded} equations.

Inference:  how to get from current state to next state?  Big problem!!  New ground facts have to be read off from satisfaction of all equations.  Rather intractable...

See Chapter \ref{chap:background1}.

How to enumerate conclusions?  The loss function (\ref{loss-function}) can be trained on any data with correlations.  But what we have is data in the form of time series.  Need extra measures to ensure that equations only model $x_t \rightarrow x_{t+\Delta t}$.  Now how to enumerate conclusions?  From current state, iterate over all equations to generate new states.  This is getting very close to the classical logic-based AI inference algorithm.

The \textbf{interestingness function} gives a probability distribution over conclusions given the current ``context'' (which we identify as the current state $x_t$): $ \mathrm{Intg}(\delta x) = \mathbb{P}(\delta x | x_t) $.  This function has to be learned.  It has an equivariant structure due to the state as a set of propositions with permutation invariance.

One more efficiency problem:  iterating through all equations is inefficient, which brings back the necessity of the classical \textbf{rete algorithm}:  instead of matching rules against the state, we should match the current state against rules.  In other words, instead of $\delta x := \bigcup_i \mathrm{rule}_i (x)$, perform $\delta x := \text{compiled-rules}(\Delta x) $, where $\Delta x$ is the change of the current state from the previous state, and $\delta x$ is the change from the current state to the next state.

But we may also avoid the rete algorithm by stacking logical equations into \textbf{layers}, thus getting an efficiency advantage similar to deep learning.  A ``single'' step of logic inference would mean going through multiple layers of logic rules (equations).

\section{``Geometric'' logic inference algorithm}

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{geometric-logic-algorithm.png}}}
\end{equation}

\begin{itemize}
	\item What is a logic fact within a state?
	\item How does a rule generate a single new fact?
\end{itemize}

A fact consists of a point (in subject space) and a predicate that contains it.  The point itself does not suffice because it can belong to various predicates.

\subsection{How to determine if a rule is satisfied}

To apply a rule, each \textbf{atomic term} in the rule has to be satisfied.  For each predicate $Q()$, this is verified by testing if we have any points among the facts contained in $Q$.

If we have \textsf{father(john, pete)} as a fact then we certainly can satisfy \textsf{father}$(x,y)$.  But we already have the point \textsf{(john, pete)} which may satisfy other predicates $Q(x,y)$.  So our method is slightly more permissive (and thus more powerful) in rules matching.

How does the rule's RHS generate a new fact?  It should also be a (point, predicate) pair.  The point has to \textbf{match} the premise.  How could this be ensured?

Secondly, the output predicate may not cover the point.

The matching process:  Syntactically, we look at each literal in the rule and see if any fact unifies with the literal.  Geometrically, it means taking a point and checking if it lies inside a predicate.  If the fact is a (point, predicate) pair then it is certain that the point belongs to the predicate, so it is not necessary to check for membership.  The result is simply taking the point when the predicates match.  But we still need to keep track of which variables the point coordinates are binding to.

The matching of the second literal will also return a point, but its coordinates would be bound to different dimensions.

The binding of coordinates would be the basis of verifying the rule LHS.

RHS:  The bound variables (in various dimensions) need to be projected to the output space.  It may or may not be covered by the output predicate.  If not, the rule does \textit{not} apply.  This is in accord with the principle that rules should not change during inference.

\subsection{Handling variables in rules}

% Besides $(p, \gamma)$ we seem to need another parameter, the variable number specifying which of $\{ v_1, ..., v_n \}$ is referenced in a rule's argument.  This seems to be a parameter independent of $(p, \gamma)$.  But it seems irrelevant if there is no cylindrification?

Again drawing inspiration from Halmos' algebraic logic.  A monadic logic deals with predicates as $X \rightarrow \Omega$, whereas in polyadic logic one has $X^I \rightarrow \Omega$ where $I$ is an index set.  The elements of $I$ are \textbf{variables} even though they do not really change.  $X^I$ creates $I$ copies of the Subject Space $X$. Note that $(I \rightarrow X) \rightarrow \Omega$ is not isomorphic to $I \rightarrow (X \rightarrow \Omega)$.

% Solution:  Suppose we have a predicate of 2 arguments, eg. $\logic{loves}(\_, \_)$.  Train a neural network $P: X^I \rightarrow X^I$.  Train a ``selector'' function which chooses 2 out of $I$, in other words, a soft top-2 function $s: I \rightarrow \mathbb{R}$.  This gives a probability distribution over $I$, or more accurately speaking, a fuzzy-value distribution over $I$.  To evaluate a predicate $P$ at a point $(x,y)$, in other words to obtain the truth value of $P(x,y)$, we send $(x,y)$ to $P(x,y) \in X^I$.  Then the fuzzy values are applied to each copy of $X$ in $X^I$ such that...

For example $I = \{ 1,2,3,4 \}$ would provide 4 variables or ``slots'' for a rule to use.  A rule such as:
\begin{equation}
	\logic{father}(X_1,X_2) \wedge \logic{father}(X_2,X_3) \rightarrow \logic{grand\hbox{-}father}(X_1,X_3)
\end{equation}
uses 3 variables: $X_1, X_2, X_3$.  I have used the symbol $X$ with subscripts to denote variables, as a reminder that each variable is really a \textbf{copy} of our Subject Space $X$.

% The first predicate converges to dimensions $(\#1,\#2)$, the second predicate converges to dimensions $(\#2,\#3)$.  The output predicate converges to dimensions $(\#1,\#3)$.  These dimensions are discrete numbers and thus not differentiable.  To learn them by machine-learning, one way is to make them into stochastic actions and learn their probability distribution.

It is perhaps illuminating to rewrite the logic rule this way:
\begin{equation}
	\logic{father}(X^I) \wedge \logic{father}(X^I) \rightarrow \logic{grand\hbox{-}father}(X^I)
\end{equation}
where $I$ is at least $\{ 1,2,3 \}$.

In my mind, I had this mental picture:
\vspace{-0.2cm} \begin{equation}
\logic{father}(\begin{tikzpicture} \draw[line width=3pt, cyan!70] (0.2,0.9) -- (0.2,0) (0.5,0.9) -- (0.5,0) (0.8,0.2) -- (0.8,0); \draw[thick] (0,0) to (1,0); \end{tikzpicture})
\wedge
\logic{father}(\begin{tikzpicture} \draw[line width=3pt, cyan!70] (0.2,0.2) -- (0.2,0) (0.5,0.9) -- (0.5,0) (0.8,0.9) -- (0.8,0); \draw[thick] (0,0) to (1,0); \end{tikzpicture})
\rightarrow
\logic{grand\hbox{-}father}(\begin{tikzpicture} \draw[line width=3pt, cyan!70] (0.2,0.9) -- (0.2,0) (0.5,0.2) -- (0.5,0) (0.8,0.9) -- (0.8,0); \draw[thick] (0,0) to (1,0); \end{tikzpicture})
\end{equation}
where the blue bars represent some kind of relative ``proportion'' of variables, such that the overall construction could be \textbf{differentiable}.

% Each \textbf{argument} of a predicate would be associated with a \textbf{softmax} over the index set $I$, which selects one element out of $I$.  The softmax assigns, to each $i \in I$, a weight $w_i \in \mathbb{R}$.

% WRONG:  Then we can make copies of $X$ into $X^I$ as $\{ w_1 X_1, ..., w_n X_n \}$.  But this method assumes that the Subject Space $X$ has \textbf{vector space structure} and is amenable to scalar multiplication.  As I have argued in a blog article, the space of ``word embedding'' is actually a \textbf{metric space} rather than vector space.

% WRONG: A better solution is to associate the weight to that argument of the predicate such that the resulting \textbf{true value} of that predicate would be \textbf{weakened}.  The truth value of a predicate $P$ is evaluated by sending a point $(a,b)$ as input to a \textbf{neural network} representing $P$.  Its output is a real number normalized $\in [ 0,1 ]$ and interpreted as fuzzy truth-value.

The eventual implementation is quite natural:
\begin{align}
\nonumber
& \qquad \qquad \tikzmark{Y1}
\setlength{\fboxrule}{3pt}
\fcolorbox{cyan!70}{white}{$\hat{x}^1$}
\qquad
\tikzmark{Y2} \fcolorbox{cyan!70}{white}{$\hat{x}^2$}
\qquad
\tikzmark{Y3} \fcolorbox{cyan!70}{white}{$\hat{x}^3$} \\
& \qquad w_{ij}^k \\
& \nonumber \\
& \logic{father}(\tikzmark{X11} x_{11}, \tikzmark{X12} x_{12}, \tikzmark{X13} \Square_{13}) \wedge
\logic{father}(\tikzmark{X21} \Square_{21}, \tikzmark{X22} x_{22}, \tikzmark{X23} x_{23}) \rightarrow \logic{grand\hbox{-}father}(x_1,\Square_2,x_3)
\nonumber
\begin{tikzpicture}[remember picture,overlay]
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X11) +(7pt,12pt) $);
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X12) +(7pt,12pt) $);
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X13) +(7pt,12pt) $);
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X21) +(7pt,12pt) $);
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X22) +(7pt,12pt) $);
\draw (pic cs:Y1) +(10pt,-7pt) -- ($ (pic cs:X23) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X11) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X12) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X13) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X21) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X22) +(7pt,12pt) $);
\draw (pic cs:Y2) +(10pt,-7pt) -- ($ (pic cs:X23) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X11) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X12) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X13) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X21) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X22) +(7pt,12pt) $);
\draw (pic cs:Y3) +(10pt,-7pt) -- ($ (pic cs:X23) +(7pt,12pt) $);
\end{tikzpicture}
\end{align}
Each weight represents how ``strong'' an argument (lower row) occupies a variable slot (upper row).  \textbf{Softmax} normalizes the weights so that one of them would be most prominent, ie. the selected one.  Every argument can potentially appear in every variable slot, thus the bipartite graph is fully connected.

The resulting formula is this:  Each output slot $\hat{x}^k \in \hat{X}^I$ contains a \textbf{sum} of vectors from all arguments of all predicates on the left hand side of a rule:
\begin{equation}
\hat{x}^k \mathrel{:}= \underset{\text{predicates}}{\forall i \in} \; \underset{\text{arguments}}{\forall j \in} \left\langle \mathrm{soft}\max_{ij} w_{ij}^k \; , \; x_{ij} \right\rangle .
\end{equation}
Note that $ij$ is treated as one set of indices, so $x_{ij} = ( x_{11}, x_{12}, x_{13}, x_{21}, x_{22}, x_{23} )$ say, and $x_{ij}^\top$ is a simple column vector.

In the above method, the copying of a vector is not ``pure'' but may be contaminated by residuals of other vectors.  This is tolerable, as the vector embedding of tokens can allow for small perturbations.

The structure of our formula is reminiscent of the Transformer's \textbf{Self-Attention}:
\begin{equation}
\text{Attention}(\mathbf{Q,K,V}) = \text{soft}\max  \frac{\langle \mathbf{Q,K} \rangle}{\sqrt{d_k}} \mathbf{V}
\end{equation}
but softmax does not commute with inner products, so the two formulas are not equivalent.

What our formula does is to move variables around;  this suggests that the Transformer is also capable of moving \textbf{tokens} around.  The syntactic manipulation ability of Transformers has been corroborated by other researchers [cite].

For comparison, this is the \textbf{neural string diagram} for the traditional Self-Attention:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Attention-string-diagram.png}}}
\end{equation}

The training of the neural network for predicate $P$ occurs in $X^2$ if the arity of the predicate is 2.  This has nothing to do with $X^I$ as the set $I$ of variables are only involved in logic rules.

To evaluate a rule:  Each predicate, such as $\logic{father(\_,\_)}$, is matched against some points, such as $(a,b)$.  The neural network returns a TV (truth value) independent of the variable space $X^I$.  But we need to put the point $(a,b)$ into $X^I$ via the \textbf{Softmax} selector.  This is repeated for all predicates on the LHS or ``head'' of the rule.  Then the truth values will determine if the rule is satisfied or not.  If yes, we will output a predicate such as $\logic{grand\hbox{-}father}(X_1, X_3)$.  Now we go back to $X^I$ to retrieve the point corresponding to the variables $(X_1, X_3)$.

\subsection{String diagram}

\begin{equation}
\hspace{-1.5cm}\vcenter{\hbox{\includegraphics[scale=0.7]{logic-Transformer-string-diagram-2.png}}}
\end{equation}

\textbf{Legend:}
\begin{enumerate}
	\item The {\LARGE\color{magenta}\RIGHTarrow} indicates \textbf{internal data} supplied by the Transformer, similar to the Q,K,V matrices in traditional Transformers.  These are modified through ``learning'' or training.
	\item The 3 \RIGHTarrow's \ are \textbf{inputs} to the Transformer, this is matched with the 3 \textbf{outputs} on the right of the diagram.
	\item Tags {\color{orange}$[M,N,...]$} indicate the \textbf{dimensions} of data streams.
\end{enumerate}

\textbf{Remarks:}
\begin{enumerate}  % [label=\protect\circled{\arabic*}]
	\item[\textbullet] The \textbf{grayed-out} parts of the circuit tries to match the \textbf{predicates} of Working-Memory propositions to those in the rules.  For example, the predicate $\logic{loves}$ where $\logic{loves(John,Mary)}$ matches $\logic{loves}(X_1,X_2)$.  But I take the more liberal approach where matching succeeds whenever a point in Working Memory falls within the target predicate's region of definition, regardless of whether the source and target predicates match or not.  For example, assume $\logic{human(Socrates)}$ is in Working Memory, and we want to match against $\logic{mortal(X_1)}$.  This will succeed if the $\logic{human}$ region lies strictly within the $\logic{mortal}$ region in Subject Space.  If the predicate regions are learned properly, this would be the case.  Using this approach, rules are more easily satisfied, and the code seems to be simpler.

	\item[\circled{1}] We want to compare arguments of predicates in rules against propositions from Working Memory, ie, $\gamma(C) \stackrel{?}{=} c$, where $\gamma$ is a cylindrification factor (turned into a radial basis function), $C$ is a \textbf{constant} in a predicate in a rule, such as $\logic{Mary}$ in $\logic{loves}(\logic{John},\logic{Mary})$, to be matched against $c$, another constant argument of a predicate of a proposition from Working Memory.  In the above example, $(\logic{John},\logic{Mary})$ is a \textbf{point} in the Subject Space $X^2$.  $\logic{John}$ and $\logic{Mary}$ are also known as ``coordinates'' in $X^2$.

	\item[\circled{2}] On each step, we're not just comparing two elements, but two multi-dimensional matrices of elements.  On the left side is a matrix of dimensions $[M,K,|X|^I]$, for $M$ rules, each with $K$ predicates, each with $I$ argument slots, each containing an element of the Subject Space $X$.  On the right side (from Working Memory), we have a matrix of dimensions $[N,|X|^I]$, for $N$ propositions, each with $I$ argument slots, each containing an element of the Subject Space $X$. \\

	The operation $\stackrel{?}{=}$ is performed on the axis $I$, common to both sides.  For each rule $m \in M$, for each predicate $k \in K$, for each slot $i \in I$, we compare $C_{mki}$ against $c_{ni}$ for each proposition $n \in N$, for each argument slot $i \in I$.  This totals to $MKNI$ comparisons (the index $i$ counts only once).  This means trillions of comparisons, by our naive estimation of sizes.

	\item[\circled{3}] Each proposition in Working Memory has a truth value.  This TV has to be multiplied to the result in \circled{2}.
\end{enumerate}

The operation $a \stackrel{?}{=} b$ can be implemented by the function $(x \stackrel{?}{=} y) \coloneq y \cdot \sigmoid(x) + 1 - \sigmoid(x)$, where $\sigmoid$ is the sigmoid function such as $\frac{1}{1 + \exp(-100x + 50)}$.
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{selector-2D-plot.png}}}
\qquad
\vcenter{\hbox{\includegraphics[scale=0.7]{selector-3D-plot.png}}}
\end{equation}

A trick often used in homotopy is: $h_t(x) = t \cdot f(x) + (1-t) \cdot g(x), t \in [0,1]$, so the function $h_t(x)$ is ``continuously deformed'' from $f(x)$ to $g(x)$.

We want to adjust the probability $p$ with weight $w$, such that
\begin{eqnarray}
\mbox{if } w = 0 \mbox{ or close to } 0, & \mbox{output } p = 1 \\
\mbox{if } w = 1 \mbox{ or close to } 1, & \mbox{output } p = p
\end{eqnarray}
Formula: output = $p \cdot t + 1 \cdot (1-t)$, this is the 'homotopy' trick where $t = \mbox{sigmoid}(w)$, specifically, $t = 1/(1 + \exp(-c \cdot (w - 0.5)))$ where $c$ is a scaling factor to make the sigmoid more steep and the sigmoid is shifted to where the midpoint occurs at w = 1/2.

\section{Computer representation of rules}

As explained in the Introduction (\S\ref{Ch0_logic_rules}), our rules are of the form:
\begin{equation}
\begin{aligned}
\mbox{rule 1:} & \quad \boxed{P^1_{1} X^1_{11}... X^1_{1I}} \;\wedge\; \boxed{P^1_2 X^1_{21} ... X^1_{2I}} \;\wedge\;...\; \boxed{P^1_K X^1_{K1} ... X^1_{KI}} \rightarrow \boxed{P^1_0 X^1_{01} ... X^1_{0I}} \\
\mbox{rule 2:} & \quad \boxed{P^2_{1} X^2_{11}... X^2_{1I}} \;\wedge\; \boxed{P^2_2 X^2_{21} ... X^2_{2I}} \;\wedge\;...\; \boxed{P^2_K X^2_{K1} ... X^2_{KI}} \rightarrow \boxed{P^2_0 X^2_{01} ... X^2_{0I}} \\
.... & \quad .... \\
\mbox{rule M:} & \quad \boxed{P^M_{1} X^M_{11}... X^M_{1I}} \;\wedge\; \boxed{P^M_2 X^M_{21} ... X^M_{2I}} \;\wedge\;...\; \boxed{P^M_K X^M_{K1} ... X^M_{KI}} \rightarrow \boxed{P^M_0 X^M_{01} ... X^M_{0I}}
	\raisetag{20pt}
\end{aligned}
\end{equation}

Each logic rule may vary in the following ways:
\begin{enumerate}
	\item number of literals and their polarity
	\item number of arguments
	\item each argument may be a logic variable or logic constant
	\item which specific logic variable or constant.
\end{enumerate}
The numbers in \#1 and \#2 can be fixed.

For \#3 and \#4, each variable or constant may be represented by a pair $(p, \gamma)$ where $p$ is a point (or coordinates) in the Subject Space, and $\gamma$ is the \textbf{cylindrification factor} representing the degree to which this argument is like a point or like a ``for all'' variable.  % But this is problematic as the dimension of the Subject Space $X$ is not the same as the dimension of the variable space $\{ X_1, X_2, ... , X_n \}$.

Each predicate has as its arguments $n$ copies of the Subject Space, ie, $X^n$.  One or more of the copies may be \textbf{cylindrified}.  The meaning of the cylindrical factor $\gamma$ is illustrated as follows:
%There are two senses of the word ``cylindrify.''  Think for example the change from (John, Mary) to ($x$, Mary) as in ``everyone loves Mary.''  In the first sense, the point John on the $x$-coordinate becomes ``everyone,'' ie, the entire $x$-dimension becomes a cylinder.  In the second sense, a cylinder appears on the $y$-axis at the position of Mary:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{cylindrify-example.png}}}
\end{equation}
In the above example, $(X,Y)$ are two copies of the Subject Space $X$, each is 1-dimensional.

During rule matching, suppose a point $p = (a,b)$ is presented from a proposition $P(p) = P(a,b)$ in the state $X$.  This is matched against a term $Q(X,Y)$ in the rule.  The rule-matching process seems costly, which suggests we may need to re-formulate a neural-network version of the \textbf{Rete algorithm} -- but such an algorithm would not be differentiable.  In fact, differentiability dictates that all rules must participate in matching at all times (or at least probabilistically with non-zero probabilities).

Anyway, the coordinate $a$ needs to be matched against the argument $X_1$, which may be a variable or a constant.  Denote the constant part of $X_1$ as $C_1$, so we try the matching $a \stackrel{?}{=} \gamma(C_1)$, where $\gamma$ is the cylindrification, a radial basis function.  This returns the truth value of the match.  At the same time, $a$ would be copied into the variable slot $X_1$ for substitution processing, independently.  This is OK, as the result will have a low TV if matching fails.

% The dimension $n$ is not just the number of variables appearing in one predicate in a rule, but the total number of variables appearing in a rule.

\vspace{1cm} \hrule \vspace{1cm}

We suggest the following values (for a general human-level intelligent agent):
\begin{equation}
\begin{tabular}{|c|c|c|}
	\hline
	\textbf{Meaning} & \textbf{Symbol} & \textbf{Est. typical values} \\
	\hline
	\# of rules & $M$ &  \sim millions \\
	\hline
	total \# of predicates & $K$ &  10,000's \\
	\hline
	\# of predicates per rule & $J$ &  2-10, typical 3,4 \\
	\hline
	\# of arguments per predicate & $I$ & 2, 3, 4  \\
	\hline
	Subject Space dimension & $|X|$ &  100-1000, typical 512, 768 \\
	\hline
	Predicate Space dimension & $|\Psi|$ &  same as above \\
	\hline
	Hidden layer size & $h$ & similar to $|X|$ \\
	\hline
	\# of propositions in working memory & $N$ & \sim 100 \\
	\hline
\end{tabular}
\end{equation}

Total number of parameters (in descending order of size) $= MKIX + MK\Psi + MKI^2 + MKI + 2MK + MI^2 + NIX + N\Psi + N + K(|X|+(L-2)h) \approx $ 20 trillion.  Perhaps $M$ and $K$ have to be reduced.

All the parameters (rules) of the Logic Transformer:
\begin{equation}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Parameters} & \textbf{Dimensions} & \textbf{??} \\
\hline
$K$ predicates & $M$ rules, $K$ predicates, $|\Psi|$ & \\

\end{tabular}
\end{equation}

\section{``Higher-order'' logic}

The first-order Logic Transformer was simple to understand as a logic, but limited in expressive power as a logic.  As Ben Goertzel pointed out (during an online discussion with the author), it does not have mechanisms to handle \textbf{Skolem functions} and thus existential quantifiers.

One way to get around this problem is to define $\exists$ via a set of higher-order logic rules.  Without explicitly specifying how to do so (though I believe this can be done), we simply use a more powerful ``higher-order'' logic in the sense that rules admit substitutions in all (predicate or argument) positions, and hope that machine learning will learn the required axioms.

The new rules are of the form (without the special designation of ``predicates''):
\begin{equation}
\begin{aligned}
\mbox{rule 1:} & \quad \boxed{X^1_{11}... X^1_{1I}} \;\wedge\; \boxed{X^1_{21} ... X^1_{2I}} \;\wedge\;...\; \boxed{X^1_{K1} ... X^1_{KI}} \rightarrow \boxed{X^1_{01} ... X^1_{0I}} \\
\mbox{rule 2:} & \quad \boxed{X^2_{11}... X^2_{1I}} \;\wedge\; \boxed{X^2_{21} ... X^2_{2I}} \;\wedge\;...\; \boxed{X^2_{K1} ... X^2_{KI}} \rightarrow \boxed{X^2_{01} ... X^2_{0I}} \\
.... & \quad .... \\
\mbox{rule M:} & \quad \boxed{X^M_{11}... X^M_{1I}} \;\wedge\; \boxed{X^M_{21} ... X^M_{2I}} \;\wedge\;...\; \boxed{X^M_{K1} ... X^M_{KI}} \rightarrow \boxed{X^M_{01} ... X^M_{0I}}
\end{aligned}
\end{equation}

This gives us the revised string diagram:
\begin{equation}
\hspace{-1cm}\vcenter{\hbox{\includegraphics[scale=0.7]{logic-Transformer-string-diagram-3.png}}}
\end{equation}

\section{Rules recommender}

The rules recommender would be a set function:
\begin{equation}
\logic{Ru}: \{ \mbox{current state} \} \rightarrow \{ \mbox{set of rules} \}
\end{equation}
which has equivariant structure on both its input and output.  This suggests the \textbf{Transformer} architecture is suitable for learning this function.

But this is clearly \textbf{non-differentiable}!

\subsection{Differentiability}

In binary logic a rule either applies or does not apply.  To make the entire set of rules differentiable, rule application must be ``graded''.

So we propose a \textbf{probabilistic} rule-matching mechanism:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{rules-recommender.png}}}
\end{equation}
Rules are mapped by a function $\rho$ to the state space, such that rules are located close to the states they are most likely to match (this is a multi-objective optimization problem).  Given a current state $x$, denote the ball around it as $B(x, d)$.  The rules we predict most likely to match $x$ are given by $\rho^{-1}(B(x,d))$ for some distance $d$.  We can assign a Gaussian probability distribution centered around $x$ over such rules, so that rules are selected \textbf{probabilistically} for update.

It seems that doing so would not affect the convergence (of the learning algorithm) of the rules, but it would be much more efficient as the probability distribution is \textit{more} concentrated around $x$, so that large numbers of rules need \textit{not} be evaluated (for each state, at each time step).

% Rule 產生了新的結論，但先前選擇的 rule 是否影響新的 context？
% 其實 rules 本身可以分層。 最高的 rules 選擇下層的某些 rules。  但這只是其中一種做法。
% 其實如果有某 decision tree 負責 rules 的分流，似乎也仍然是可微的?
% but if the leafs of the decision tree are SHARED, differentiability may be a problem?
% the sorting tree can be deterministic... how does that affect differentiability?
% perhaps the rules at a whole can be regarded as a single function transforming the current state to the next state.  Differentiability would be discussed with respect to this function.
% What does it mean for differentiability to be broken...?  The total rule function must change infinitesimally per each update...
% differentiation is like dy/dx.... when x = the total function...  the question is whether we can always perform x+dx... where dx will not be of finite length
% the state x will change from step to step....  but that's not a problem...  we care about the

\begin{equation}
x_{t+1} = \squareF(x_t; \Theta)
\end{equation}
where $\Theta$ are all the parameters that determine the rules, $\squareF$ is the total function aggregating all the rules.  If we vary any one rule parameter, does it change abruptly?  The parameters $\Theta$ include the rules-sorting function $\rho$.

For gradient descent we need to calculate $\nabla_\Theta \mathcal{L}$ where $\Theta$ are all the parameters of rules, and the loss function is defined as the sum of errors over all rules, $\displaystyle \mathcal{L} = \sum_{\text{all rules}} \epsilon^2 $.  Each error usually is given by the difference as compared against a reference value (supervised learning), but in our case such a value is unavailable, instead the objective comes from reinforcement learning, ie, the Hamilton-Jacobi-Bellman equation:  $\displaystyle J(x_{t+1}) = \max_a [ \gamma J(x_t) + R(x_t,a) ] $, where $a$ means \textbf{action}.  In the logic-based setting, an action means applying a logic rule to change the state.

Policy function:
\begin{equation}
\pi: X \times A \rightarrow [0,1]
\end{equation}
By the \textbf{Policy Gradient Theorem}, calculation of $\nabla_\Theta J$ translates to calculating $\nabla_\Theta \pi$.

In reinforcement learning, in particular the \textbf{Policy Gradient} algorithm, we need to calculate the gradient $\nabla_\phi \mathcal{L}(\phi)$ of a loss function of the form:
\begin{equation}
\mathcal{L}(\phi) = \mathbb{E}_{z \sim q_\phi(z)}[ f(z) ] .
\end{equation}
The expectation gives an integral $\displaystyle \nabla_\phi \mathcal{L}(\phi) = \nabla_\phi \int dz q_\phi(z) f(z)$ which removes the ``randomness'' and evaluates to a real number.  This allows to be handled by traditional differential calculus.

At each state, the application of all rules results in a list of all available actions.  The differentiability problem may arise from probabilistic rule-matching.

\section{Interestingness}

This can be implemented implicitly by making the inference algorithm output a probability distribution over all deduced conclusions and then picking the most probable one.

\section{Combining logic and reinforcement learning}

\subsection{Logical policy function}

The policy function $\pi: X \times A \rightarrow [0,1]$ should be implemented with logic structure.  The current state would be matched against rules selected by the rules recommender, and then each rule would be applied, resulting in conclusions $Z_i$ each associated with a probabilistic strength $\mathbb{P}(Z_i)$.  These are the available actions and their probabilities as given by the policy.

\vspace{1cm} \hrule \vspace{1cm}

This section concerns how to establish the ``link'' from logic to reinforcement learning.

For reinforcement learning, I find it easier to consider Q-learning, ie, learning the utility function $Q(s,a)$, where $s$ = current state and $a$ = action taken in current state.

During forward inference, each logic rule of the form (also known as Horn form):
\begin{equation}
	P \wedge Q \wedge R \wedge ... \rightarrow Z
\end{equation}
yields a conclusion $Z$.  But the ``truths'' of the premises $P,Q,R,..$ are not binary but fuzzy, because the rules have to be differentiable (this can be implemented by softmax).  Thus each conclusion $Z$ is also fuzzy.  The application of all the rules yields a set of conclusions $\{ Z_i \}$, with their associated fuzzy truth values, which we can normalize as a probability distribution over all available next actions.  This can be regarded as the \textbf{policy function} $\pi(a|s)$, which gives the probability of an action given the current state, $\mathbb{P}(a|s)$.

In Q-learning we need to learn the values $Q(s,a)$.  Given the reward $R$, $Q(s,a)$ satisfies the Bellman equation:
\begin{equation}
	Q(s,a) \mathrel{+}= \eta \big( R + \gamma \max_{a'} Q(s',a') - Q(s,a) \big)
\end{equation}
where $\eta$ is the learning rate, $\gamma$ is the discount rate of values.

The policy function $\pi(a|s)$ is not exactly the same as $Q(s,a)$, but there is a trick that can relate them, that is the basis of \textbf{Soft-Q learning} (Soft Actor-Critic, SAC):
\begin{equation}
	\pi(a|s) \propto \exp Q(s,a)
\end{equation}

The situation can be illustrated by the following figure (not a commutative diagram):
\begin{equation}
\begin{tikzcd}[column sep=1.5cm, row sep=0.6cm]
	{} & {} & \parbox{1.5cm}{\linespread{-0.5}\selectfont\centering Bellman update} \arrow[d] \\
	\parbox{1.2cm}{\linespread{-0.5}\selectfont\centering logic rules} \arrow[r]
	& \pi(a|s) \arrow[r,shift left=2pt,harpoon,"\log"]
	& Q(s,a) \arrow[l, shift left=2pt, harpoon, "\exp"]
\end{tikzcd}
\end{equation}
Logic rules give us $\pi(a|s)$, which in turn gives us $Q(s,a)$, but $Q(s,a)$ is also determined by Bellman update.

My solution is:  Logic outputs a finite set of actions, to each action is associated a weight, which we identify as $Q$.  Use Bellman update to train these $Q$ values.  To choose an action, calculate $\pi(a|s)$ as suggested by the Soft-Q method.  This has the effect of \textbf{maximum-entropy} exploration.

\section{Basics of DQN (Deep Q Learning)}

The \textbf{Bellman optimality condition} says that:
\begin{equation}
	Q^*_t = \max_{a} \{ R + \gamma Q^*_{t+1} \}
\end{equation}
where $()^*$ denotes optimal values.

In the \textbf{Bellman update},
\begin{equation}
	Q_t \mathrel{+}= \eta \cdot \mbox{TD-error}
\end{equation}
where TD = \textbf{temporal difference} is defined as:
\begin{equation}
	\mbox{TD-error} = \overbrace{R + \gamma \max_{a'} Q(s_{t+1},a')}^{\mbox{ideal value}} \overbrace{ - \; Q(s,a)}^{\mbox{current value}} .
\end{equation}
So obviously the Bellman update causes the current value of $Q$ to \textbf{converge} to that of the optimal value $Q^*$.

Remember that, in the simplest \textbf{Q-table} method, we apply the Bellman update over Q-table entries.  The \textbf{DQN} approach differs in that, instead of a Q-table, we use a deep neural network in its place.  So the updating of the Q-table now becomes updating the DQN via \textbf{gradient descent}.
